{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#张量 tensor 就是 多维数组 多维列表 阶：tensor的维度  0维 就是一个数字  \n",
    "\n",
    "a = tf.constant([1,2])\n",
    "b = tf.constant([3,4])\n",
    "\n",
    "result = a + b\n",
    "print(result)\n",
    "\n",
    "'''\n",
    "这就是一个计算图  只描述过程  不运算\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 计算图：描述一个简单的神经传递\n",
    "\n",
    "x = tf.constant([[1,2]])\n",
    "w = tf.constant([[3],[4]])\n",
    "\n",
    "y = tf.matmul(x,w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11]]\n"
     ]
    }
   ],
   "source": [
    "# 绘画session：执行计算图中的节点运算\n",
    "x = tf.constant([[1,2]])\n",
    "w = tf.constant([[3],[4]])\n",
    "\n",
    "y = tf.matmul(x,w)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化参数 ————随机生成\n",
    "\n",
    "w = tf.Variable(tf.random_normal([2,3],stddev=2,mean=0,seed=1))  \n",
    "#生成一个随机权重 random_normal 是代表正态分布 [2,3]是生成的形状 stddv是标准差  mean是均值  seed随机种子\n",
    "\n",
    "#  tf.truncated_normal() 去除偏离值的正态分布\n",
    "#  tf.random_uniform()  平均分布\n",
    "\n",
    "#还有——tf.zeros全零  tf.ones全1  tf.fill定值数组  tf.constant直接给指"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]] w2 [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "[[3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "#  前向传播\n",
    "'''\n",
    "神经网络：\n",
    "    准备数据集，提取特征，喂给网络\n",
    "    搭建NN，输入到输出，先搭建计算图然后执行session（前向传播）\n",
    "    迭代优化参数（反向传播）\n",
    "    使用模型预测\n",
    "    \n",
    "'''\n",
    "\n",
    "#定义输入和参数（直接输入 or 占位符）\n",
    "#x = tf.constant([[0.7,0.5]])\n",
    "#x = tf.placeholder(tf.float32,shape=(1,2))\n",
    "#可以用占位符,用了占位符就要在最后run的时候喂数据\n",
    "\n",
    "#假如我们不知道要喂多少组数据\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "#下面feed_dict 就可以随意增加样本数量了\n",
    "\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "#定义前向传播过程\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#用session计算\n",
    "with  tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print('w1',sess.run(w1),\"w2\",sess.run(w2))\n",
    "    #print(sess.run(y))\n",
    "    #print(sess.run(y,feed_dict={x:[[0.7,0.5]]}))\n",
    "    print(sess.run(y,feed_dict={x:[[0.7,0.5],[0.2,0.3],[0.3,0.4],[0.4,0.5]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]] [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n",
      "w1 [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]] w2 [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "当前loss值 5.1311817\n",
      "当前loss值 0.42911103\n",
      "当前loss值 0.40978912\n",
      "当前loss值 0.3999228\n",
      "当前loss值 0.39414558\n",
      "当前loss值 0.39059657\n",
      "w1 [[-0.7000663   0.9136318   0.08953571]\n",
      " [-2.3402493  -0.14641267  0.58823055]] w2 [[-0.06024267]\n",
      " [ 0.91956186]\n",
      " [-0.0682071 ]]\n"
     ]
    }
   ],
   "source": [
    "# 反向传播\n",
    "'''\n",
    "利用梯度下降，减少损失函数\n",
    "\n",
    "loss：预测值跟实际答案的差距\n",
    "\n",
    "均方误差MSE\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y,y^))\n",
    "\n",
    "反向传播：以最小化loss为优化目标(三个下降算法，)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "神经网络：\n",
    "    准备数据集，提取特征，喂给网络\n",
    "    搭建NN，输入到输出，先搭建计算图然后执行session（前向传播）\n",
    "    迭代优化参数（反向传播）\n",
    "    使用模型预测\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 8\n",
    "seed = 23455\n",
    "\n",
    "#基于seed生成随机数\n",
    "rng =  np.random.RandomState(seed)\n",
    "\n",
    "# 随机数返回32 行 2列 的矩阵  表示32组 体积和重量  作为输入数据集\n",
    "X = rng.rand(32,2)\n",
    "\n",
    "#如果小于1给Y赋值 1(合格)  如果不小于1给Y赋值0（不合格）   （逻辑判断）\n",
    "#结果作为数据集的标签(答案)\n",
    "Y = [[int(x0+x1<1)] for (x0,x1) in X]\n",
    "print(X,Y)\n",
    "\n",
    "#       1-定义神经网络输入、参数、输出，定义前向传播过程\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#       2-定义loss函数以及反向传播\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.MomentumOptimizer(learning_rate).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "#       3-生成session，训练steps轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #输出目前未被训练的参数 w1 w2\n",
    "    print('w1',sess.run(w1),'w2',sess.run(w2))\n",
    "    print('\\n')\n",
    "    \n",
    "    #训练模型\n",
    "    steps = 3000\n",
    "    for i in range(steps):\n",
    "        start = (i*batch_size)%32  #这里就是minibatch的用法\n",
    "        end = start + batch_size\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "        if i %500 ==0:\n",
    "            total_loss = sess.run(loss,feed_dict={x:X,y_:Y})\n",
    "            print('当前loss值',total_loss)\n",
    "    \n",
    "    #训练结束后再次输出参数\n",
    "    print('w1',sess.run(w1),'w2',sess.run(w2))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "准备：\n",
    "    import 模块\n",
    "    常量定义\n",
    "    生成数据集\n",
    "    \n",
    "前向传播：\n",
    "    定义输入 x=  y_=\n",
    "    定义参数 w1=  w2=\n",
    "    定义输出  a= y=\n",
    "    \n",
    "反向传播：定义损失函数、反向传播优化方法\n",
    "    loss = \n",
    "    train_step = SGD MGD Adam\n",
    "    \n",
    "生成session 迭代：\n",
    "    with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    steps = \n",
    "    for i in range(steps):\n",
    "        strat =\n",
    "        end = \n",
    "        sess.run(train_step,feed_dict=:{})\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
