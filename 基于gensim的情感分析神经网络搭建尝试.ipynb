{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预训练模型包含单词总数： 50102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['的', '</s>', '在', '是', '年', '和', '了', '於', '為', '有']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "zh_model = KeyedVectors.load_word2vec_format('zh.vec')\n",
    "words = []\n",
    "for word in zh_model.vocab:\n",
    "    words.append(word)\n",
    "print(\"预训练模型包含单词总数： {}\".format(len(words)))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量维度: 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50102"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"词向量维度: {}\".format(\n",
    "    len(zh_model[words[0]])\n",
    "))\n",
    "len(words)\n",
    "#print(\"单词“{}”的向量表示: {}\".format(words[0], zh_model[words[0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 搞笑, Similarity: 0.48\n",
      "Word: 笑, Similarity: 0.47\n",
      "Word: 白痴, Similarity: 0.46\n",
      "Word: 害羞, Similarity: 0.45\n",
      "Word: 笨蛋, Similarity: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for similar_word in zh_model.similar_by_word('傻', topn=5):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n首先把微博文本打标签  关联微博url \\n然后做成dataframe 两列  一列是x 一列是 y\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  神经网络部分\n",
    "'''\n",
    "首先把微博文本打标签  关联微博url \n",
    "然后做成dataframe 两列  一列是x 一列是 y\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y\n",
    "'''\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "'''\n",
    "\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\n",
    "\n",
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], label_to_emoji(int(pred[i])))\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "\n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "\n",
    "\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "\n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "\n",
    "    for j in range(m):                       # Loop over training examples\n",
    "\n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        words = X[j].lower().split()\n",
    "\n",
    "        # Average words' vectors\n",
    "        avg = np.zeros((50,))\n",
    "        for w in words:\n",
    "            avg += word_to_vec_map[w]\n",
    "        avg = avg/len(words)\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备数据集\n",
    "\n",
    "df = pd.read_excel('D:/语料1.xls')\n",
    "df = df[['text','tag']]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"D:/税微博7月1-10改.xlsx\")\n",
    "df2 = df2[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#世界杯赌球#来来来交交智商税07月01日00:59来自三星GALAX\n",
      "打分：\n",
      "杜兰特可能选择1+1，明年夏天有选择权跳出，并且有鸟权可以签5年顶薪。简单说就是杜兰特损失18-19的500万，从3500万变成3000万，因为签1+1只有前一年120%的涨幅，换回明年5年顶薪的机会。顺便下赛季勇士省点税，和多签人没关系07月01日00:57来自Rozier’siPhone客\n",
      "打分：\n",
      "中国房价最高城市开征空置税！雷霆手段控楼市即将到来？|地产·杆(分享自@Flipboard红板报)O网页链接07月01日00:57来自iPhone客\n",
      "打分：\n",
      "宁波市“月荷税莲”名家书法展于月至日在鼓楼展出，本次展出由海曙国税局主办，欢迎参观。07月01日00:57来自微博weibo.c\n",
      "打分：\n",
      "【2014年共3.94万辆新能源汽车免征购置税】据国家税务总局统计，2014年9月-12月有29个省市有办理新能源汽车免征车购税手续，共免征车购税新能源汽车3.94万辆，其中乘用车3.38万辆，商用车5615辆。07月01日00:54来自小米Note3拍人\n",
      "打分：\n",
      "天惹，可爱！//@H-HeeBum王大拿:我被可爱死了//@Auroraborealis-:啊太可爱了//@SnowRiver秘密日報社:必須上可愛稅！//@Alpha帅锤:可爱//@Loki夫人:请上可爱税谢谢//@VelvetBobby:软fufu//@Wally不吃咸鸭蛋:太可爱了07月01日00:54来自iPhone8Pl\n",
      "打分：\n",
      "⭐应届生求职招聘简历五险一金养老保险医疗保险生育保险工伤保险失业保险公积金贷款买房面试技巧签合同三方协议社保报销工资薪酬年终奖月薪年薪绩效业绩指标谈判劳动仲裁毕业生档案报到证登记证单身税失业证朱正廷@THEO-朱正廷07月01日00:53来自倚栈的Andro\n",
      "打分：\n",
      "//@Vera南夏:呜哇//@卉子_ForPaul:呜呜呜呜可爱哭了！！！//@MissyWow:我的妈我要这个男人！//@汤抖森的猫耳:啊啊啊嘤嘤嘤嘤嘤嘤嘤嘤嘤(ಥ_ಥ)//@Alpha帅锤:可爱//@Loki夫人:请上可爱税谢谢//@VelvetBobby:软fufu//@Wally不吃咸鸭蛋:太可爱了07月01日00:52来自iPhone客\n",
      "打分：\n",
      "可爱(๑•.•๑)//@Alpha帅锤:可爱//@Loki夫人:请上可爱税谢谢//@VelvetBobby:软fufu//@Wally不吃咸鸭蛋:太可爱了07月01日00:52来自Andro\n",
      "打分：\n"
     ]
    }
   ],
   "source": [
    "#打分小程序\n",
    "df2 = df2[:10]\n",
    "df2['fenshu']= None\n",
    "def dafen(df):\n",
    "    for i in range(1,len(df)):\n",
    "        print(df.loc[i][0])\n",
    "        fenshu = input('打分：')\n",
    "        df.loc[i]['fenshu'] = fenshu\n",
    "        \n",
    "dafen(df2[:10\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>fenshu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#世界杯赌球#来来来交交智商税07月01日00:59来自三星GALAX</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>杜兰特可能选择1+1，明年夏天有选择权跳出，并且有鸟权可以签5年顶薪。简单说就是杜兰特损失1...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中国房价最高城市开征空置税！雷霆手段控楼市即将到来？|地产·杆(分享自@Flipboard红...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>宁波市“月荷税莲”名家书法展于月至日在鼓楼展出，本次展出由海曙国税局主办，欢迎参观。07月0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>【2014年共3.94万辆新能源汽车免征购置税】据国家税务总局统计，2014年9月-12月有...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>天惹，可爱！//@H-HeeBum王大拿:我被可爱死了//@Auroraborealis-:...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>⭐应届生求职招聘简历五险一金养老保险医疗保险生育保险工伤保险失业保险公积金贷款买房面试技巧签...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>//@Vera南夏:呜哇//@卉子_ForPaul:呜呜呜呜可爱哭了！！！//@MissyW...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text fenshu\n",
       "1                #世界杯赌球#来来来交交智商税07月01日00:59来自三星GALAX   None\n",
       "2  杜兰特可能选择1+1，明年夏天有选择权跳出，并且有鸟权可以签5年顶薪。简单说就是杜兰特损失1...   None\n",
       "3  中国房价最高城市开征空置税！雷霆手段控楼市即将到来？|地产·杆(分享自@Flipboard红...   None\n",
       "4  宁波市“月荷税莲”名家书法展于月至日在鼓楼展出，本次展出由海曙国税局主办，欢迎参观。07月0...   None\n",
       "5  【2014年共3.94万辆新能源汽车免征购置税】据国家税务总局统计，2014年9月-12月有...   None\n",
       "6  天惹，可爱！//@H-HeeBum王大拿:我被可爱死了//@Auroraborealis-:...   None\n",
       "7  ⭐应届生求职招聘简历五险一金养老保险医疗保险生育保险工伤保险失业保险公积金贷款买房面试技巧签...   None\n",
       "8  //@Vera南夏:呜哇//@卉子_ForPaul:呜呜呜呜可爱哭了！！！//@MissyW...   None"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[:8]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['感觉到幸福' '无限扩大' '坚定不移' '中二垃圾少年' '分享视频' '简直泪目' '情绪失控' '为他跳起毛利战舞' '最后一个真喜庆'\n",
      " '又帅又有才' '斗气' '晒太阳这么舒服' '这位女士去了动物园' '陪伴和温暖' '接触陶瓷已经有二十年' '最强美男子' '将该男生抓获'\n",
      " '发生了不好的事' '感到同情' '该男生心理有问题' '去年买的衣服' '纯真自由的生活' '不让自己过于后悔' '家暴不是犯错' '更加珍惜'\n",
      " '十分恐惧' '伤心难过的时候' '安慰他们' '给予他们支持' '一个好消息' '去芬兰的视频' '上学' '经典的存在' '增加幸福感'\n",
      " '羊毛裙' '海马毛珊瑚绒家居服' '收集温暖、治愈、幸运消息的版块' '没抛弃你' '组织学生撤离' '慌乱之中' '相信友情' '他是个好人'\n",
      " '斗罗大陆' '转发抽奖' '他真的很努力' '领取本月星座运势秘籍' '父母老去' '信任和照顾' '谣言' '不要错过'\n",
      " '考名牌大学是所有家长的希' '活泼开朗' '能把科技唠成家常' '花很多时间' '经历人间的生离死别' '明年出场的新本子' '继续保持可爱'\n",
      " '出现过很多次' '最喜爱的作家' '消防员将呼吸器让给婴儿' '同一个世界' '吃瓜群众' '当众掌掴辱骂3名学生' '闻风而逃'\n",
      " '更多孩子在欧洲各地营地被陆续发现' '人类之耻' '最完美的血统' '压抑的怒气' '在众人的簇拥下游街示众' '令挪威人害怕'\n",
      " '只能忍受更加无常的命运' '最幸福的日子' '仇恨' '战争仍在继续' '上面刻着他的名字' '大屠杀资料馆' '对拍摄的居民身份证相片不满意'\n",
      " '心地善良' '突然刷爆朋友圈' '梦见好多好多三明治'] [ 1  0  1 -2  0 -1 -2  0  1  1 -1  1  0  2  0  1 -1 -1 -1 -2  0  1  1 -2\n",
      "  1 -1 -1  1  2  2  0  0  1  2  0  0  2 -1  0 -1  1  2  0  0  1  0 -1  2\n",
      " -1 -1  1  2  0  0 -2  0  1  0  1  0  0 -1 -2 -1  0 -2  2 -2 -2 -1 -1  2\n",
      " -2 -1  0 -2 -1  1  0  0] ['今晚起11点半前睡觉' '第一次火冒三丈' '网友们这下五味杂陈' '激动倒地身亡' '巨精彩' '公开支持' '生在职场身不由己'\n",
      " '暴力殴打' '有罪的' '安静漂亮' '熟视无睹' '眼含笑意' '男生喜欢女生' '优秀' '后青春期的诗' '非常惊艳'\n",
      " '我正坐在回家的火车' '永远不会' '为大家提供多款游戏' '身份证照可重拍'] [ 0 -1 -1 -2  2  2 -1 -2 -1  2 -1  2  1  2  0  2  0 -1  0  0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train = np.array(df['text'])[20:]\n",
    "Y_train = np.array(df['tag'])[20:]\n",
    "X_test = np.array(df['text'])[:20]\n",
    "Y_test = np.array(df['tag'])[:20]\n",
    "\n",
    "print(X_train,Y_train,X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = '2'\n",
    "Y = np.eye(5)[int(Y)+2]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25859  ,  0.22439  , -0.050759 , -0.043929 , -0.12326  ,\n",
       "       -0.13298  ,  0.38223  ,  0.028844 , -0.45969  , -0.11023  ,\n",
       "       -0.28477  ,  0.033635 ,  0.24709  ,  0.096075 ,  0.19565  ,\n",
       "        0.25969  , -0.14721  , -0.38603  ,  0.33876  ,  0.32904  ,\n",
       "       -0.1708   , -0.15786  , -0.025603 ,  0.059944 , -0.33972  ,\n",
       "       -0.099513 ,  0.080611 ,  0.73935  ,  0.20882  ,  0.22658  ,\n",
       "        0.40455  , -0.022722 ,  0.33032  , -0.24389  ,  0.26036  ,\n",
       "        0.30444  , -0.42477  ,  0.23229  ,  0.28836  ,  0.34982  ,\n",
       "        0.19008  ,  0.15437  , -0.17395  , -0.035459 , -0.083979 ,\n",
       "       -0.10536  ,  0.49609  ,  0.08135  , -0.07996  , -0.38033  ,\n",
       "       -0.18951  , -0.11024  , -0.018996 , -0.24443  , -0.13062  ,\n",
       "       -0.19577  , -0.1743   ,  0.25094  ,  0.12162  ,  0.18115  ,\n",
       "       -0.23824  ,  0.26407  , -0.38566  , -0.1113   ,  0.25321  ,\n",
       "        0.10368  ,  0.099715 , -0.060418 , -0.10433  , -0.14365  ,\n",
       "       -0.0098679, -0.52156  ,  0.081901 ,  0.037713 ,  0.031883 ,\n",
       "       -0.36728  ,  0.40347  , -0.062547 ,  0.13073  ,  0.062564 ,\n",
       "       -0.25737  , -0.080853 ,  0.048382 , -0.18479  ,  0.49318  ,\n",
       "        0.51433  , -0.11771  ,  0.25038  ,  0.43303  , -0.11838  ,\n",
       "       -0.32486  , -0.12166  , -0.10999  ,  0.085845 ,  0.13526  ,\n",
       "       -0.16492  , -0.011024 ,  0.079461 , -0.089157 , -0.16082  ,\n",
       "        0.56102  ,  0.45517  , -0.23104  ,  0.067382 , -0.006989 ,\n",
       "        0.033899 , -0.25465  , -0.50012  ,  0.093566 ,  0.2017   ,\n",
       "       -0.075186 ,  0.076664 , -0.22433  ,  0.081688 ,  0.21242  ,\n",
       "       -0.089736 , -0.082674 ,  0.5501   , -0.07267  , -0.19915  ,\n",
       "        0.084893 ,  0.096908 ,  0.13278  ,  0.051223 , -0.1824   ,\n",
       "       -0.21149  , -0.35015  ,  0.35535  ,  0.18146  ,  0.11332  ,\n",
       "        0.088309 ,  0.12234  , -0.15015  ,  0.28586  , -0.10785  ,\n",
       "       -0.19009  ,  0.15631  , -0.082936 , -0.021148 ,  0.41231  ,\n",
       "        0.41083  ,  0.15571  ,  0.1442   ,  0.10565  ,  0.17062  ,\n",
       "       -0.11744  ,  0.18476  ,  0.26789  ,  0.17661  , -0.124    ,\n",
       "        0.092518 ,  0.19313  ,  0.30925  ,  0.69772  ,  0.068685 ,\n",
       "       -0.0018109,  0.3626   , -0.41297  ,  0.24782  , -0.25264  ,\n",
       "        0.0015632, -0.28368  ,  0.2755   , -0.087836 ,  0.019309 ,\n",
       "       -0.20657  ,  0.42664  ,  0.071397 ,  0.012401 , -0.30223  ,\n",
       "       -0.20024  , -0.1028   ,  0.26768  ,  0.20576  ,  0.015183 ,\n",
       "       -0.26244  , -0.18297  ,  0.020963 , -0.24487  , -0.30025  ,\n",
       "        0.023233 , -0.15534  ,  0.45017  , -0.061562 ,  0.083301 ,\n",
       "        0.21061  ,  0.20973  ,  0.15042  ,  0.38839  , -0.31793  ,\n",
       "       -0.19345  ,  0.058468 ,  0.41232  , -0.037071 , -0.27731  ,\n",
       "       -0.1591   ,  0.39125  ,  0.46579  , -0.10809  ,  0.064534 ,\n",
       "        0.1664   , -0.33558  ,  0.19213  ,  0.070176 , -0.46213  ,\n",
       "        0.27932  , -0.074577 , -0.24433  ,  0.14726  ,  0.41137  ,\n",
       "        0.17356  , -0.12699  , -0.18126  ,  0.042894 ,  0.4008   ,\n",
       "       -0.13044  , -0.049861 , -0.60008  ,  0.27162  ,  0.070603 ,\n",
       "       -0.065067 , -0.35787  ,  0.21802  , -0.35422  ,  0.29987  ,\n",
       "       -0.24188  ,  0.22076  ,  0.081917 ,  0.099983 , -0.056292 ,\n",
       "        0.22033  ,  0.11645  ,  0.36119  ,  0.14315  ,  0.14868  ,\n",
       "        0.14563  , -0.44685  ,  0.15008  ,  0.27024  ,  0.15048  ,\n",
       "       -0.22231  , -0.52935  , -0.02804  , -0.031306 ,  0.10441  ,\n",
       "        0.041469 ,  0.031828 ,  0.39115  ,  0.57942  , -0.61561  ,\n",
       "       -0.18211  , -0.1334   ,  0.12623  ,  0.15923  , -0.091196 ,\n",
       "       -0.13147  , -0.0075795,  0.021498 , -0.59805  ,  0.25234  ,\n",
       "        0.24475  , -0.044441 , -0.12296  , -0.037363 ,  0.44755  ,\n",
       "        0.16485  , -0.010949 , -0.032923 ,  0.70893  ,  0.1341   ,\n",
       "       -0.18168  , -0.19774  , -0.63169  , -0.29086  , -0.12402  ,\n",
       "       -0.016642 ,  0.040816 , -0.019189 ,  0.18127  , -0.21856  ,\n",
       "       -0.017941 , -0.085679 ,  0.18633  ,  0.17719  , -0.27874  ,\n",
       "        0.037291 ,  0.11458  , -0.011578 ,  0.043249 ,  0.071962 ,\n",
       "        0.088215 ,  0.19936  , -0.30835  ,  0.39031  , -0.19531  ,\n",
       "       -0.29     , -0.17956  , -0.35067  ,  0.2936   ,  0.2745   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '搞笑'\n",
    "zh_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "#把句子的向量均值化\n",
    "\n",
    "sen = '我超级喜欢纯子啊'\n",
    "def sen_to_avg(sen):\n",
    "    sen =re.sub(r\"[\\s+\\.\\!\\/_,$%^*(【】：\\]\\[\\-:;+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+|[0-9]+\",\"\",sen)\n",
    "            \n",
    "    l = jieba.lcut(sen)\n",
    "    #print(l)\n",
    "    avg = np.zeros((300,))\n",
    "    n = 0\n",
    "    for w in l:\n",
    "        if w in words:\n",
    "            avg += zh_model[w]\n",
    "        else:\n",
    "            avg = avg\n",
    "            n = n+1\n",
    "    if len(l)-n == 0:\n",
    "        avg = avg\n",
    "    else:\n",
    "        avg = avg/(len(l)-n)\n",
    "    #print(len(l)-n)\n",
    "    return avg\n",
    "        \n",
    "    \n",
    "#把标签转化为 one—hot \n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[int(Y)+2]\n",
    "    return Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, W, b):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "\n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "\n",
    "    for j in range(m):                       # Loop over training examples\n",
    "\n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        avg = sen_to_avg(X[j])\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)-2\n",
    "        print(pred[j])\n",
    "        \n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\Temp\\jieba.cache\n",
      "Loading model cost 0.935 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.6302944374124193\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "Accuracy: 0.3375\n",
      "Epoch: 100 --- cost = 0.036406346748235285\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[2.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[2.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "Accuracy: 0.9375\n",
      "Epoch: 200 --- cost = 0.011717578878123937\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-2.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[2.]\n",
      "[-2.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[2.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "Accuracy: 0.975\n",
      "Epoch: 300 --- cost = 0.005958321669746401\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-2.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[2.]\n",
      "[-2.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[-1.]\n",
      "[2.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.]\n",
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "# 模型部分\n",
    "\n",
    "def model(X,Y,learning_rate = 0.01,num_iterations = 400):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    m = Y.shape[0]  #训练集数量\n",
    "    n_y = 5   #标签的个数  softmax的类别数\n",
    "    n_h = 300  #词向量的维度数\n",
    "    \n",
    "    W = np.random.randn(n_y,n_h)/np.sqrt(n_h)\n",
    "    \n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            \n",
    "            \n",
    "            avg = sen_to_avg(X[i])\n",
    "            #print(i,avg)\n",
    "            #前向\n",
    "            z = np.dot(W,avg)+b\n",
    "            a = softmax(z)\n",
    "            \n",
    "            \n",
    "            #计算损失\n",
    "            Y_oh = convert_to_one_hot(Y[i],n_y)\n",
    "            #print(Y_oh)\n",
    "            cost = -np.sum(Y_oh * np.log(a))\n",
    "            \n",
    "            \n",
    "            #下降参数\n",
    "            dz = a - Y_oh\n",
    "            dw = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            #反向传播\n",
    "            W = W - learning_rate*dw\n",
    "            b = b - learning_rate*db\n",
    "            \n",
    "        if t%100 ==0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b)\n",
    "            \n",
    "    return W,b\n",
    "W,b = model(X_train,Y_train,num_iterations = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[2.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[1.]\n",
      "[1.]\n",
      "[-1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-2.]\n",
      "[1.]\n",
      "[0.]\n",
      "[-1.]\n",
      "[0.]\n",
      "[0.]\n",
      "Accuracy: 0.45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [-2.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [-2.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict(X_test,Y_test,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
